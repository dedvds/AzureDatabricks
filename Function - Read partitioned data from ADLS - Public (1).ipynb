{"cells":[{"cell_type":"code","source":["import datetime\nfrom pyspark.sql.functions import rand, randn, lit, col\n\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["#Generate some data per day\n#numdays=400\n#base = datetime.datetime.strptime(\"2017-10-23\",\"%Y-%m-%d\")\n#date_list = [base - datetime.timedelta(days=x) for x in range(numdays)]\n#ugly way of generating data with For loop, Write away per Y/M/D\n# for date in date_list:\n  \n#   temp_df = sqlContext.range(0, 10)\n#   temp_date=date\n# # Generate two other columns using uniform distribution and normal distribution.\n#   temp_df=temp_df.select(\"id\", lit(temp_date).alias(\"date\"), randn(seed=27).alias(\"data\"))\n#   temp_df.write.format('com.databricks.spark.csv') \\\n#   .mode('overwrite').option(\"header\", \"true\")\\\n#   .save(\"/mnt/partitioning/new/\"+str(temp_date.year)+\"/\"+str(temp_date.month)+\"/\"+str(temp_date.day)+\"/test.csv\")\n  \n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":2},{"cell_type":"code","source":["#Read simulated data to detect schema\ndf=spark.read.format('csv').options(header='true', inferSchema='true').load(\"mnt/partitioning/new/2019/1/1/*.csv\")\n\n#define Schema\ndf_schema=df.schema"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":3},{"cell_type":"code","source":["startdate=\"2016-11-30\"\nenddate=\"2019-2-26\""],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":4},{"cell_type":"code","source":["#Function to read data efficiently from subdirectories. Seems to be faster than load.where() approach, \ndef read_efficient(startdate,enddate,directory,schema):\n #Construct Year Range\n  yearrange=range(datetime.datetime.strptime(startdate,\"%Y-%m-%d\").year\\\n                  ,datetime.datetime.strptime(enddate,\"%Y-%m-%d\").year+1)\n  #Extract month of startdate and enddate\n  [minmonth,maxmonth]=[datetime.datetime.strptime(startdate,\"%Y-%m-%d\").month,\\\n                       datetime.datetime.strptime(enddate,\"%Y-%m-%d\").month]\n  #Extract day of startdate and enddate\n  [minday,maxday]= [datetime.datetime.strptime(startdate,\"%Y-%m-%d\").day,datetime.datetime.strptime(enddate,\"%Y-%m-%d\").day]\n  #Construct 5 seperate dataframes (if existing): \n  # - Rest of days of month of startdate\n  # - Rest of months of year of startdate\n  # - Middle Full years\n  # - Rest of months of year of enddate\n  # - Rest of days of last month of enddate\n  \n  #Remaining days of month of startdate:\n  temp_begin_day=spark.read.format('csv').\\\n    options(header='true',schema=schema).\\\n    load(str(directory)+str(list(yearrange)[0])+\"/\"+str(minmonth)+\"/\"+'{' + ','.join(map(str, list(range(minday,32)))) + '}'+\"/*.csv\")\n  \n  #If statement to check if there are full months left of year of startdate, if not, pass empty DF\n  if minmonth!=12:\n    temp_begin_month=spark.read.format('csv').\\\n      options(header='true',schema=schema).\\\n      load(str(directory)+str(list(yearrange)[0])+\"/\"+'{' + ','.join(map(str, list(range(minmonth+1,13)))) + '}'+\"/*/*.csv\")\n  else:\n    temp_begin_month=spark.createDataFrame([], schema)\n  \n  #If statement to check if there are full months left of year of enddate, if not, pass empty DF\n  if maxmonth !=1:\n    temp_end_month=spark.read.format('csv').\\\n      options(header='true',schema=schema).\\\n      load(str(directory)+str(list(yearrange)[-1])+\"/\"+'{' + ','.join(map(str, list(range(1,maxmonth)))) + '}'+\"/*/*.csv\")\n  else:\n    temp_end_month=spark.createDataFrame([], schema)\n    \n  #Remaining days of month of enddate:  \n  temp_end_day=spark.read.format('csv').\\\n    options(header='true',schema=df_schema).\\\n    load(str(directory)+str(list(yearrange)[-1])+\"/\"+str(maxmonth)+\"/\"+'{' + ','.join(map(str, list(range(1,maxday)))) + '}'+\"/*.csv\")\n  \n  #If statement to check if there are full years between year startdate and year enddate, if not, pass empty DF\n  if list(yearrange)[1:-1]!=[]:\n    temp_middle=spark.read.format('csv').\\\n      options(header='true',schema=df_schema).\\\n      load(str(directory)+'{' + ','.join(map(str, list(yearrange[1:-1]))) + '}'+\"/*/*/*.csv\")\n    return(temp_begin_day.union(temp_begin_month.union(temp_middle.union(temp_end_month.union(temp_end_day)))))\n  else:\n    return(temp_begin_day.union(temp_begin_month.union(temp_end_month.union(temp_end_day))))\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":5},{"cell_type":"code","source":["df=read_efficient(\"2018-12-03\",\"2019-01-12\",\"mnt/partitioning/new/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">2018-12-03 2019-01-12\n12 1\n3 12\n</div>"]}}],"execution_count":6},{"cell_type":"code","source":["from pyspark.sql.functions import year, count\n(df\n    .groupBy(year(\"date\").alias(\"year\"))\n    .agg(count(\"data\").alias(\"count\"))\n    .show())"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">+----+-----+\nyear|count|\n+----+-----+\n2018|  290|\n2019|  110|\n+----+-----+\n\n</div>"]}}],"execution_count":7},{"cell_type":"code","source":["#Different approach to read all, then filter\n\nspark.read.format('csv').options(header='true',schema=df_schema).load(\"mnt/partitioning/new/*/*/*/*.csv\").\\\n  where(col('date').between(datetime.datetime.strptime(startdate,\"%Y-%m-%d\"), datetime.datetime.strptime(enddate,\"%Y-%m-%d\"))).explain()"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">== Physical Plan ==\n*(1) Project [id#30839, date#30840, data#30841]\n+- *(1) Filter ((isnotnull(date#30840) &amp;&amp; (date#30840 &gt;= 2017-10-23 00:00:00)) &amp;&amp; (date#30840 &lt;= 2019-11-18 00:00:00))\n   +- *(1) FileScan csv [id#30839,date#30840,data#30841] Batched: false, DataFilters: [isnotnull(date#30840), (date#30840 &gt;= 2017-10-23 00:00:00), (date#30840 &lt;= 2019-11-18 00:00:00)], Format: CSV, Location: InMemoryFileIndex[dbfs:/mnt/partitioning/new/2017/10/1/test.csv, dbfs:/mnt/partitioning/new/2017/..., PartitionFilters: [], PushedFilters: [IsNotNull(date), GreaterThanOrEqual(date,2017-10-23 00:00:00), LessThanOrEqual(date,2019-11-18 0..., ReadSchema: struct&lt;id:string,date:string,data:string&gt;\n</div>"]}}],"execution_count":8}],"metadata":{"name":"Function - Read partitioned data from ADLS - Public","notebookId":4298128862048488},"nbformat":4,"nbformat_minor":0}
