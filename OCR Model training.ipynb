{"cells":[{"cell_type":"code","source":["from pyspark.sql import *\nfrom pyspark.ml.classification import LogisticRegression\n"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":1},{"cell_type":"code","source":["trainDF=spark.read.parquet(\"/mnt/adls2/trainset.parquet/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">---------------------------------------------------------------------------</span>\n<span class=\"ansired\">Py4JJavaError</span>                             Traceback (most recent call last)\n<span class=\"ansigreen\">&lt;command-4428870687339165&gt;</span> in <span class=\"ansicyan\">&lt;module&gt;</span><span class=\"ansiblue\">()</span>\n<span class=\"ansigreen\">----&gt; 1</span><span class=\"ansiyellow\"> </span>trainDF<span class=\"ansiyellow\">=</span>spark<span class=\"ansiyellow\">.</span>read<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&quot;/mnt/adls2/trainset.parquet/&quot;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/readwriter.pyc</span> in <span class=\"ansicyan\">parquet</span><span class=\"ansiblue\">(self, *paths)</span>\n<span class=\"ansigreen\">    307</span>         <span class=\"ansiyellow\">[</span><span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;name&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;string&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;year&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;int&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;month&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;int&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">(</span><span class=\"ansiblue\">&apos;day&apos;</span><span class=\"ansiyellow\">,</span> <span class=\"ansiblue\">&apos;int&apos;</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">]</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    308</span>         &quot;&quot;&quot;\n<span class=\"ansigreen\">--&gt; 309</span><span class=\"ansiyellow\">         </span><span class=\"ansigreen\">return</span> self<span class=\"ansiyellow\">.</span>_df<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_jreader<span class=\"ansiyellow\">.</span>parquet<span class=\"ansiyellow\">(</span>_to_seq<span class=\"ansiyellow\">(</span>self<span class=\"ansiyellow\">.</span>_spark<span class=\"ansiyellow\">.</span>_sc<span class=\"ansiyellow\">,</span> paths<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    310</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    311</span>     <span class=\"ansiyellow\">@</span>ignore_unicode_prefix<span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/java_gateway.py</span> in <span class=\"ansicyan\">__call__</span><span class=\"ansiblue\">(self, *args)</span>\n<span class=\"ansigreen\">   1158</span>         answer <span class=\"ansiyellow\">=</span> self<span class=\"ansiyellow\">.</span>gateway_client<span class=\"ansiyellow\">.</span>send_command<span class=\"ansiyellow\">(</span>command<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1159</span>         return_value = get_return_value(\n<span class=\"ansigreen\">-&gt; 1160</span><span class=\"ansiyellow\">             answer, self.gateway_client, self.target_id, self.name)\n</span><span class=\"ansigreen\">   1161</span> <span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">   1162</span>         <span class=\"ansigreen\">for</span> temp_arg <span class=\"ansigreen\">in</span> temp_args<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/pyspark/sql/utils.pyc</span> in <span class=\"ansicyan\">deco</span><span class=\"ansiblue\">(*a, **kw)</span>\n<span class=\"ansigreen\">     61</span>     <span class=\"ansigreen\">def</span> deco<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     62</span>         <span class=\"ansigreen\">try</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">---&gt; 63</span><span class=\"ansiyellow\">             </span><span class=\"ansigreen\">return</span> f<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">*</span>a<span class=\"ansiyellow\">,</span> <span class=\"ansiyellow\">**</span>kw<span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     64</span>         <span class=\"ansigreen\">except</span> py4j<span class=\"ansiyellow\">.</span>protocol<span class=\"ansiyellow\">.</span>Py4JJavaError <span class=\"ansigreen\">as</span> e<span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">     65</span>             s <span class=\"ansiyellow\">=</span> e<span class=\"ansiyellow\">.</span>java_exception<span class=\"ansiyellow\">.</span>toString<span class=\"ansiyellow\">(</span><span class=\"ansiyellow\">)</span><span class=\"ansiyellow\"></span>\n\n<span class=\"ansigreen\">/databricks/spark/python/lib/py4j-0.10.6-src.zip/py4j/protocol.py</span> in <span class=\"ansicyan\">get_return_value</span><span class=\"ansiblue\">(answer, gateway_client, target_id, name)</span>\n<span class=\"ansigreen\">    318</span>                 raise Py4JJavaError(\n<span class=\"ansigreen\">    319</span>                     <span class=\"ansiblue\">&quot;An error occurred while calling {0}{1}{2}.\\n&quot;</span><span class=\"ansiyellow\">.</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">--&gt; 320</span><span class=\"ansiyellow\">                     format(target_id, &quot;.&quot;, name), value)\n</span><span class=\"ansigreen\">    321</span>             <span class=\"ansigreen\">else</span><span class=\"ansiyellow\">:</span><span class=\"ansiyellow\"></span>\n<span class=\"ansigreen\">    322</span>                 raise Py4JError(\n\n<span class=\"ansired\">Py4JJavaError</span>: An error occurred while calling o302.parquet.\n: com.microsoft.azure.datalake.store.ADLException: Error getting info for file /trainset.parquet\nError fetching access token\nOperation null failed with exception java.net.MalformedURLException : no protocol: 72f988bf-86f1-41af-91ab-2d7cd011db47\nLast encountered exception thrown after 5 tries. [java.net.MalformedURLException,java.net.MalformedURLException,java.net.MalformedURLException,java.net.MalformedURLException,java.net.MalformedURLException]\n [ServerRequestId:null]\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getExceptionFromResponse(ADLStoreClient.java:1169)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:737)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:718)\n\tat com.databricks.adl.AdlFileSystem.getFileStatus(AdlFileSystem.java:390)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:643)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1$$anonfun$apply$15.apply(DatabricksFileSystemV2.scala:640)\n\tat com.databricks.s3a.S3AExeceptionUtils$.convertAWSExceptionToJavaIOException(DatabricksStreamUtils.scala:107)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:640)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2$$anonfun$getFileStatus$1.apply(DatabricksFileSystemV2.scala:640)\n\tat com.databricks.logging.UsageLogging$$anonfun$recordOperation$1.apply(UsageLogging.scala:313)\n\tat com.databricks.logging.UsageLogging$$anonfun$withAttributionContext$1.apply(UsageLogging.scala:188)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat com.databricks.logging.UsageLogging$class.withAttributionContext(UsageLogging.scala:183)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionContext(DatabricksFileSystemV2.scala:400)\n\tat com.databricks.logging.UsageLogging$class.withAttributionTags(UsageLogging.scala:221)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.withAttributionTags(DatabricksFileSystemV2.scala:400)\n\tat com.databricks.logging.UsageLogging$class.recordOperation(UsageLogging.scala:298)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.recordOperation(DatabricksFileSystemV2.scala:400)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystemV2.getFileStatus(DatabricksFileSystemV2.scala:639)\n\tat com.databricks.backend.daemon.data.client.DatabricksFileSystem.getFileStatus(DatabricksFileSystem.scala:210)\n\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1426)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:775)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$16.apply(DataSource.scala:410)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$16.apply(DataSource.scala:410)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:381)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:344)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:409)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:281)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:269)\n\tat org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:684)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:295)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:226)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.net.MalformedURLException: no protocol: 72f988bf-86f1-41af-91ab-2d7cd011db47\n\tat java.net.URL.&lt;init&gt;(URL.java:593)\n\tat java.net.URL.&lt;init&gt;(URL.java:490)\n\tat java.net.URL.&lt;init&gt;(URL.java:439)\n\tat com.microsoft.azure.datalake.store.oauth2.AzureADAuthenticator.getTokenSingleCall(AzureADAuthenticator.java:237)\n\tat com.microsoft.azure.datalake.store.oauth2.AzureADAuthenticator.getTokenCall(AzureADAuthenticator.java:207)\n\tat com.microsoft.azure.datalake.store.oauth2.AzureADAuthenticator.getTokenUsingClientCreds(AzureADAuthenticator.java:63)\n\tat com.microsoft.azure.datalake.store.oauth2.ClientCredsTokenProvider.refreshToken(ClientCredsTokenProvider.java:39)\n\tat com.microsoft.azure.datalake.store.oauth2.AccessTokenProvider.getToken(AccessTokenProvider.java:36)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getAccessToken(ADLStoreClient.java:1036)\n\tat com.microsoft.azure.datalake.store.HttpTransport.makeSingleCall(HttpTransport.java:177)\n\tat com.microsoft.azure.datalake.store.HttpTransport.makeCall(HttpTransport.java:91)\n\tat com.microsoft.azure.datalake.store.Core.getFileStatus(Core.java:655)\n\tat com.microsoft.azure.datalake.store.ADLStoreClient.getDirectoryEntry(ADLStoreClient.java:735)\n\t... 42 more\n</div>"]}}],"execution_count":2},{"cell_type":"code","source":["lr = LogisticRegression(maxIter=100, regParam=0.01, elasticNetParam=0.8)\n\n# Fit the model\nlrModel = lr.fit(trainDF)\n\n# Print the coefficients and intercept for multinomial logistic regression\nprint(\"Coefficients: \\n\" + str(lrModel.coefficientMatrix))\nprint(\"Intercept: \" + str(lrModel.interceptVector))\n\ntrainingSummary = lrModel.summary\n\n# Obtain the objective per iteration\nobjectiveHistory = trainingSummary.objectiveHistory\nprint(\"objectiveHistory:\")\nfor objective in objectiveHistory:\n    print(objective)\n\n# for multiclass, we can inspect metrics on a per-label basis\nprint(\"False positive rate by label:\")\nfor i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n    print(\"label %d: %s\" % (i, rate))\n\nprint(\"True positive rate by label:\")\nfor i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n    print(\"label %d: %s\" % (i, rate))\n\nprint(\"Precision by label:\")\nfor i, prec in enumerate(trainingSummary.precisionByLabel):\n    print(\"label %d: %s\" % (i, prec))\n\nprint(\"Recall by label:\")\nfor i, rec in enumerate(trainingSummary.recallByLabel):\n    print(\"label %d: %s\" % (i, rec))\n\nprint(\"F-measure by label:\")\nfor i, f in enumerate(trainingSummary.fMeasureByLabel()):\n    print(\"label %d: %s\" % (i, f))\n\naccuracy = trainingSummary.accuracy\nfalsePositiveRate = trainingSummary.weightedFalsePositiveRate\ntruePositiveRate = trainingSummary.weightedTruePositiveRate\nfMeasure = trainingSummary.weightedFMeasure()\nprecision = trainingSummary.weightedPrecision\nrecall = trainingSummary.weightedRecall\nprint(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"code","source":["truePositiveRate"],"metadata":{},"outputs":[],"execution_count":4},{"cell_type":"code","source":["trainingSummary = lrModel.summary\nroc = trainingSummary.truePositiveRateByLabel\nplt.plot(roc['FPR'],roc['TPR'])\nplt.ylabel('False Positive Rate')\nplt.xlabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()\nprint('Training set areaUnderROC: ' + str(trainingSummary.areaUnderROC))"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["truePositives=trainingSummary.truePositiveRateByLabel"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["display([list('0123456789'),truePositives].toDF())"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["testDF=spark.read.parquet('/mnt/adls/testset.parquet')"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["predictions=lrModel.transform(testDF)"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["from pyspark.ml.evaluation import MulticlassClassificationEvaluator\nevaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"code","source":["!pip install Keras\n#!pip install tensorflow\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K"],"metadata":{},"outputs":[],"execution_count":11},{"cell_type":"code","source":[""],"metadata":{},"outputs":[],"execution_count":12}],"metadata":{"name":"OCR Model training","notebookId":4428870687339163},"nbformat":4,"nbformat_minor":0}