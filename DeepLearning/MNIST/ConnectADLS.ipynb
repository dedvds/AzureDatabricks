{"cells":[{"cell_type":"markdown","source":["This example notebook closely follows the [Databricks documentation](https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-datalake.html) for how to set up Azure Data Lake Store as a data source in Databricks."],"metadata":{}},{"cell_type":"markdown","source":["### 0) Prerequisites\n\nBefore we can load the data, we need to take care of two things:\n*Give our Databricks workspace permission to access the data lake where our data resides\n*Create a place where we can manage our secrets/credentials, so we don't have to put our credentials in the notebooks"],"metadata":{}},{"cell_type":"code","source":["a=1"],"metadata":{},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["###0.1) Get service credentials:\n<ul>\n  <li> Client ID `<aaaaaaaa-bbbb-cccc-dddd-eeeeeeeeeeee>` and Client Credential `<NzQzY2QzYTAtM2I3Zi00NzFmLWI3MGMtMzc4MzRjZmk=>`. Follow the instructions in [Create service principal with portal](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal). </li> \n<li> Get directory ID `<ffffffff-gggg-hhhh-iiii-jjjjjjjjjjjj>`: This is also referred to as *tenant ID*. Follow the instructions in [Get tenant ID](https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal#get-tenant-id). </li> \n<li> If you haven't set up the service app, follow this [tutorial](https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse). Set access at the root directory or desired folder level to the service or everyone.</li> \n    </ul>"],"metadata":{}},{"cell_type":"markdown","source":["### 0.2) Create Secret Scope\n<ul>\n<li> Next, Create the secret scope via the Databricks CLI, accessible through the Azure Bash CLI: https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-cli-from-azure-cloud-shell </li>\n<li> Configure the secrets scope within the Databricks CLI: https://docs.databricks.com/user-guide/secrets/example-secret-workflow.html </li>\n<li> Fill in the Defined secret scope and secrets to connect to the adls with hidden credentials </li>\n  </ul>"],"metadata":{}},{"cell_type":"markdown","source":["### 1) Mount ADLS\nNext, we mount ALDS to the DBFS file path, so all user of the cluster can have access to the data. (in Python)\nwe first set variables with predefined secrets. Next, we mount the ADLS to a folder of own choice,i.e. /mnt/MNIST/"],"metadata":{}},{"cell_type":"code","source":["%python\nclientid = dbutils.preview.secret.get(scope = \"scopename\", key = \"clientid\")\ncredential = dbutils.preview.secret.get(scope = \"scopename\", key = \"adlskeys\")\nrefreshurl = dbutils.preview.secret.get(scope = \"scopename\", key = \"tenantid\")\n\n    # Mount the ADLS\nconfigs = {\"dfs.adls.oauth2.access.token.provider.type\": \"ClientCredential\",\n       \"dfs.adls.oauth2.client.id\": clientid,\n       \"dfs.adls.oauth2.credential\": credential,\n       \"dfs.adls.oauth2.refresh.url\": refreshurl}\n\ndbutils.fs.mount(\n       source = \"adl://<name of adls>.azuredatalakestore.net/\",\n       mount_point = \"/mnt/MNIST/\",\n       extra_configs = configs)"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"><span class=\"ansired\">Out[</span><span class=\"ansired\">2</span><span class=\"ansired\">]: </span>True\n</div>"]}}],"execution_count":7},{"cell_type":"markdown","source":["We can list our mounted data sources with:"],"metadata":{}},{"cell_type":"code","source":["%fs ls /mnt/"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th></tr></thead><tbody><tr><td>dbfs:/mnt/MNIST/</td><td>MNIST/</td><td>0</td></tr><tr><td>dbfs:/mnt/training-sources/</td><td>training-sources/</td><td>0</td></tr></tbody></table></div>"]}}],"execution_count":9},{"cell_type":"markdown","source":["In case we want to unmount, we can do that with:"],"metadata":{}},{"cell_type":"code","source":["dbutils.fs.unmount(\"/mnt/MNIST/\")"],"metadata":{},"outputs":[{"metadata":{},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">/mnt/adls2/ has been unmounted.\nres13: Boolean = true\n</div>"]}}],"execution_count":11},{"cell_type":"markdown","source":["###2) Direct Access\n\nWith Spark configs, the Azure Data Lake Store settings can be specified per notebook. To keep things simple, the example below includes the credentials in plaintext. However, we strongly discourage you from storing secrets in plaintext. Instead, we recommend storing the credentials as [Databricks Secrets](https://docs.azuredatabricks.net/user-guide/secrets/index.html#secrets-user-guide).\n\n**Note:** `spark.conf` values are visible only to the DataSet and DataFrames API. If you need access to them from an RDD, refer to the [documentation](https://docs.azuredatabricks.net/spark/latest/data-sources/azure/azure-datalake.html#access-azure-data-lake-store-using-the-rdd-api)."],"metadata":{}},{"cell_type":"code","source":["spark.conf.set(\"dfs.adls.oauth2.access.token.provider.type\", \"ClientCredential\")\nspark.conf.set(\"dfs.adls.oauth2.client.id\", \"<ApplicationIDfromRegisteredApp>\")\nspark.conf.set(\"dfs.adls.oauth2.credential\", \"<Keys generated from app>\")\nspark.conf.set(\"dfs.adls.oauth2.refresh.url\", \"https://login.microsoftonline.com/<Tenant-ID>/oauth2/token\")"],"metadata":{},"outputs":[],"execution_count":13},{"cell_type":"code","source":["val df = spark.read.parquet(\"adl://<Name of ADLS>.azuredatalakestore.net/<Directory of files>\")\n\ndbutils.fs.ls(\"adl://<Name of ADLS>.azuredatalakestore.net/<Directory of files>\")"],"metadata":{},"outputs":[],"execution_count":14}],"metadata":{"name":"Connect to ADLS Gen 1 with or without Hidden Credentials - Public","notebookId":4018385156795507},"nbformat":4,"nbformat_minor":0}